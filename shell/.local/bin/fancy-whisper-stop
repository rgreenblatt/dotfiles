#!/usr/bin/env python3
"""Stop recording, split long audio into chunks, transcribe with whisper, merge with LLM, and type the result.

This is an enhanced version of whisper-record-stop that handles long recordings by:
1. Detecting silence and notifying if no audio detected
2. Splitting into 29 second chunks (under whisper's 30s limit)
3. Using 5-second overlaps between chunks
4. Using Sonnet 4.5 to merge the overlapping transcripts

Usage:
  fancy-whisper-stop              # Normal usage: stop recording, transcribe, type result
  fancy-whisper-stop --test FILE  # Test mode: transcribe FILE, print result (no xdotool)
"""

import argparse
import subprocess
import os
import signal
import sys
import tempfile
import time
from pathlib import Path

_start_time = None

def _elapsed():
    """Return elapsed time since start."""
    if _start_time is None:
        return 0
    return time.time() - _start_time

def _debug(msg):
    """Print debug message with timestamp."""
    print(f"DEBUG [{_elapsed():.2f}s]: {msg}", file=sys.stderr)

RECORDING_FILE = "/tmp/whisper-recording.wav"
PID_FILE = "/tmp/whisper-recording.pid"
TRANSCRIPTIONS_DIR = "/tmp/whisper-transcriptions"
MODEL = os.environ.get("WHISPER_MODEL", os.path.expanduser("~/.local/share/whisper.cpp/ggml-large-v3-turbo.bin"))
PROMPT_FILE = os.path.expanduser("~/.config/whisper/prompt.txt")

# Segment settings - 29 seconds to stay under whisper's 30s limit
CHUNK_DURATION = 29  # seconds
OVERLAP_SECONDS = 5  # overlap between segments

# Fallback silence detection threshold (in dB, more negative = quieter)
SILENCE_THRESHOLD_DB = -40

# model for merging
MERGE_MODEL = "claude-sonnet-4-5-20250929"


def get_duration(filepath: str) -> float:
    """Get audio duration in seconds."""
    result = subprocess.run(
        ["ffprobe", "-i", filepath, "-show_entries", "format=duration", "-v", "quiet", "-of", "csv=p=0"],
        capture_output=True, text=True, check=True
    )
    return float(result.stdout.strip())


def is_silent_webrtcvad(filepath: str) -> bool:
    """Check if audio contains speech using WebRTC VAD."""
    import wave
    import webrtcvad

    vad = webrtcvad.Vad(2)  # aggressiveness 0-3, 2 is moderate

    # Read the wav file
    with wave.open(filepath, 'rb') as wf:
        sample_rate = wf.getframerate()
        num_channels = wf.getnchannels()
        sample_width = wf.getsampwidth()
        audio = wf.readframes(wf.getnframes())

    # webrtcvad needs 16-bit mono at 8000/16000/32000/48000 Hz
    if sample_rate not in (8000, 16000, 32000, 48000):
        raise ValueError(f"Unsupported sample rate: {sample_rate}")
    if num_channels != 1 or sample_width != 2:
        raise ValueError("Need 16-bit mono audio")

    # Process in 30ms frames
    frame_duration_ms = 30
    frame_size = int(sample_rate * frame_duration_ms / 1000) * 2  # 2 bytes per sample

    speech_frames = 0
    total_frames = 0
    for i in range(0, len(audio) - frame_size, frame_size):
        frame = audio[i:i + frame_size]
        total_frames += 1
        if vad.is_speech(frame, sample_rate):
            speech_frames += 1

    # Consider silent if less than 5% of frames have speech
    if total_frames == 0:
        return True
    return (speech_frames / total_frames) < 0.05


def is_silent_volume(filepath: str) -> bool:
    """Fallback: Check if audio is silent using ffmpeg volumedetect."""
    result = subprocess.run(
        ["ffmpeg", "-i", filepath, "-af", "volumedetect", "-f", "null", "/dev/null"],
        capture_output=True, text=True
    )
    output = result.stderr
    for line in output.split("\n"):
        if "mean_volume:" in line:
            try:
                db_str = line.split("mean_volume:")[1].strip().split()[0]
                mean_db = float(db_str)
                return mean_db < SILENCE_THRESHOLD_DB
            except (IndexError, ValueError):
                pass
    return False


def is_silent(filepath: str) -> bool:
    """Check if audio contains speech. Uses WebRTC VAD if available, falls back to volume."""
    try:
        import webrtcvad
        _debug("Using WebRTC VAD for silence detection")
        result = is_silent_webrtcvad(filepath)
        _debug(f"WebRTC VAD result: is_silent={result}")
        return result
    except ImportError:
        _debug("webrtcvad not available, using volume detection")
        return is_silent_volume(filepath)
    except Exception as e:
        _debug(f"WebRTC VAD failed ({e}), falling back to volume detection")
        return is_silent_volume(filepath)


def notify(message: str):
    """Send a desktop notification."""
    subprocess.run(["notify-send", "Whisper", message], check=False)


def save_transcription(text: str):
    """Save transcription to /tmp/whisper-transcriptions/ with numbered history."""
    os.makedirs(TRANSCRIPTIONS_DIR, exist_ok=True)

    # Find next number
    existing = [f for f in os.listdir(TRANSCRIPTIONS_DIR) if f.endswith('.txt') and f[:-4].isdigit()]
    if existing:
        next_num = max(int(f[:-4]) for f in existing) + 1
    else:
        next_num = 1

    # Save numbered file
    numbered_path = os.path.join(TRANSCRIPTIONS_DIR, f"{next_num:04d}.txt")
    with open(numbered_path, 'w') as f:
        f.write(text)

    # Save/overwrite latest
    latest_path = os.path.join(TRANSCRIPTIONS_DIR, "latest.txt")
    with open(latest_path, 'w') as f:
        f.write(text)

    _debug(f"Saved transcription to {numbered_path} and {latest_path}")


def split_audio(filepath: str, start: float, duration: float, output: str):
    """Extract a segment from audio file."""
    subprocess.run(
        ["ffmpeg", "-y", "-i", filepath, "-ss", str(start), "-t", str(duration), "-c", "copy", output],
        capture_output=True, check=True
    )


def transcribe(filepath: str) -> str:
    """Transcribe audio file with whisper."""
    cmd = ["whisper-cli", "-m", MODEL, "-f", filepath, "--no-timestamps", "-np"]

    if os.path.exists(PROMPT_FILE):
        with open(PROMPT_FILE) as f:
            prompt = f.read().strip()
        if prompt:
            cmd.extend(["--prompt", prompt])

    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.stdout.strip()


CHUNK_BOUNDARY_MARKER = " [chunk boundary, may overlap] "


def merge_fallback(transcripts: list[str]) -> str:
    """Fallback merge: join transcripts with boundary markers."""
    return CHUNK_BOUNDARY_MARKER.join(transcripts)


def merge_with_llm(segments: list[tuple[float, float]], transcripts: list[str]) -> str:
    """Use model (Sonnet 4.5) to merge overlapping transcript segments. Falls back to simple join on failure."""
    try:
        from anthropic import Anthropic
    except ImportError:
        print("Warning: anthropic package not installed, using fallback merge", file=sys.stderr)
        return merge_fallback(transcripts)

    api_key_path = Path("~/.anthropic_api_key_high_prio").expanduser()
    if api_key_path.exists():
        api_key = api_key_path.read_text().strip()
    else:
        api_key = os.environ.get("ANTHROPIC_API_KEY")

    if not api_key:
        print("Warning: No Anthropic API key found, using fallback merge", file=sys.stderr)
        return merge_fallback(transcripts)

    client = Anthropic(api_key=api_key)

    # Build the prompt
    segment_info = []
    for i, ((start, end), text) in enumerate(zip(segments, transcripts)):
        segment_info.append(f"Segment {i+1} ({start:.1f}s - {end:.1f}s):\n{text}")

    prompt = f"""You are merging overlapping transcript segments from a speech-to-text system.

The audio was split into overlapping segments to handle long recordings. Each segment overlaps with its neighbors by about {OVERLAP_SECONDS} seconds, which means the same words appear at the end of one segment and the beginning of the next.

Your task: Merge these segments into a single coherent transcript, removing the duplicate content at segment boundaries. Output ONLY the merged transcript text, nothing else.

IMPORTANT: Do NOT fix typos, grammar, or transcription errors. Output the text as-is from the transcripts. Only fix something if you are extremely confident it is a transcription error AND you are extremely confident what the correct word should be.

{"\n\n".join(segment_info)}"""

    _debug("=== PROMPT TO MODEL ===")
    print(prompt, file=sys.stderr)
    _debug("=== END PROMPT ===")

    try:
        _debug("Calling API...")
        response = client.messages.create(
            model=MERGE_MODEL,
            max_tokens=12000,
            messages=[{"role": "user", "content": prompt}],
        )
        result = response.content[0].text
        _debug("=== MODEL RESPONSE ===")
        print(result, file=sys.stderr)
        _debug("=== END RESPONSE ===")
        return result
    except Exception as e:
        _debug(f"Anthropic API call failed ({e}), using fallback merge")
        return merge_fallback(transcripts)


def stop_recording() -> bool:
    """Stop the recording process. Returns True if recording was stopped successfully."""
    if not os.path.exists(PID_FILE):
        return False

    with open(PID_FILE) as f:
        pid = int(f.read().strip())

    # Keep recording a bit longer to capture trailing audio
    time.sleep(0.5)

    os.unlink(PID_FILE)

    try:
        os.kill(pid, signal.SIGINT)
        # Wait for process to finish
        for _ in range(20):
            os.kill(pid, 0)  # Check if still running
            time.sleep(0.05)
    except ProcessLookupError:
        pass  # Process already dead

    return True


def pad_audio(filepath: str):
    """Pad audio with 500ms of silence at the end."""
    padded_file = "/tmp/whisper-recording-padded.wav"
    subprocess.run(["sox", filepath, padded_file, "pad", "0", "0.5"], check=True)
    os.rename(padded_file, filepath)


def transcribe_simple(filepath: str) -> str:
    """Simple transcription for short recordings."""
    cmd = ["whisper-cli", "-m", MODEL, "-f", filepath, "--no-timestamps", "-np"]

    if os.path.exists(PROMPT_FILE):
        with open(PROMPT_FILE) as f:
            prompt = f.read().strip()
        if prompt:
            cmd.extend(["--prompt", prompt])

    result = subprocess.run(cmd, capture_output=True, text=True)
    return result.stdout.strip()


def transcribe_chunked(filepath: str, duration: float) -> str:
    """Transcribe long audio by splitting into chunks and merging with LLM."""
    _debug(f"transcribe_chunked called with duration={duration:.1f}s")
    # Calculate number of chunks needed
    # Each chunk is CHUNK_DURATION seconds, with OVERLAP_SECONDS overlap
    effective_duration = CHUNK_DURATION - OVERLAP_SECONDS
    num_chunks = max(1, int((duration - OVERLAP_SECONDS) / effective_duration) + 1)
    _debug(f"Will create {num_chunks} chunks (effective_duration={effective_duration}s)")

    # Calculate segment boundaries
    segments = []
    for i in range(num_chunks):
        start = max(0, i * effective_duration)
        end = min(duration, start + CHUNK_DURATION)
        if start >= duration:
            break
        segments.append((start, end))

    _debug(f"Splitting {duration:.1f}s audio into {len(segments)} chunks")

    with tempfile.TemporaryDirectory() as tmpdir:
        # Split audio
        segment_files = []
        for i, (start, end) in enumerate(segments):
            seg_duration = end - start
            output = os.path.join(tmpdir, f"segment_{i}.wav")
            split_audio(filepath, start, seg_duration, output)
            segment_files.append(output)

        # Transcribe sequentially (parallel causes GPU/memory conflicts)
        transcripts = []
        transcribe_total_start = time.time()
        for i, seg_file in enumerate(segment_files):
            chunk_start = time.time()
            _debug(f"Transcribing chunk {i+1}/{len(segment_files)}...")
            transcripts.append(transcribe(seg_file))
            _debug(f"Chunk {i+1} took {time.time() - chunk_start:.2f}s")
        _debug(f"All transcriptions took {time.time() - transcribe_total_start:.2f}s")

    # If only one chunk, no need to merge
    if len(transcripts) == 1:
        return transcripts[0]

    # Merge with LLM
    _debug("Merging with Sonnet 4.5...")
    merge_start = time.time()
    result = merge_with_llm(segments, transcripts)
    _debug(f"Merge took {time.time() - merge_start:.2f}s")
    return result


def main():
    global _start_time
    _start_time = time.time()

    parser = argparse.ArgumentParser(description="Transcribe whisper recording with chunking support")
    parser.add_argument("--test", metavar="FILE", help="Test mode: transcribe FILE, print result (no xdotool)")
    parser.add_argument("--clipboard", action="store_true", help="Copy result to clipboard instead of typing with xdotool")
    args = parser.parse_args()

    test_mode = args.test is not None
    if test_mode:
        recording_file = args.test
        _debug(f"Test mode: processing {recording_file}")
    else:
        recording_file = RECORDING_FILE
        _debug("Starting fancy-whisper-stop")

        # Stop recording
        _debug("Stopping recording...")
        if not stop_recording():
            _debug("No recording to stop (no PID file)")
            sys.exit(1)
        _debug("Recording stopped")

    # Check recording exists and has content
    if not os.path.exists(recording_file):
        _debug("Recording file doesn't exist")
        sys.exit(1)

    file_size = os.path.getsize(recording_file)
    _debug(f"Recording file size: {file_size} bytes")
    if file_size < 1000:
        _debug("File too small, removing")
        if not test_mode:
            os.unlink(recording_file)
        sys.exit(1)

    # Check for silence before processing
    _debug("Checking for silence...")
    silence_start = time.time()
    if is_silent(recording_file):
        _debug(f"Audio is silent (took {time.time() - silence_start:.2f}s), notifying and exiting")
        if not test_mode:
            notify("No audio detected")
            os.unlink(recording_file)
        sys.exit(0)
    _debug(f"Speech detected (silence check took {time.time() - silence_start:.2f}s)")

    # Pad with silence (only in normal mode, don't modify test file)
    if not test_mode:
        _debug("Padding audio...")
        pad_audio(recording_file)

    # Get duration
    duration = get_duration(recording_file)
    _debug(f"Duration: {duration:.1f}s (chunk threshold: {CHUNK_DURATION}s)")

    # If short enough, use simple transcription
    if duration <= CHUNK_DURATION:
        _debug("Using simple transcription")
        transcribe_start = time.time()
        text = transcribe_simple(recording_file)
        _debug(f"Transcription took {time.time() - transcribe_start:.2f}s")
        _debug(f"Transcription result: {text[:100]}..." if len(text) > 100 else f"Transcription result: {text}")
    else:
        _debug("Using chunked transcription")
        text = transcribe_chunked(recording_file, duration)
        _debug(f"Merged result: {text[:100]}..." if len(text) > 100 else f"Merged result: {text}")

    # Output the result
    _debug(f"Final text length: {len(text)} chars")
    if text:
        # Save to file in test or clipboard mode
        if test_mode or args.clipboard:
            save_transcription(text)

        if test_mode:
            _debug("=== FINAL OUTPUT ===")
            print(text)
        elif args.clipboard:
            _debug("Copying to clipboard...")
            subprocess.run(["xclip", "-selection", "clipboard"], input=text.encode(), check=True)
            notify("Copied to clipboard")
            _debug("Done")
        else:
            _debug("Typing result with xdotool...")
            subprocess.run(["xdotool", "type", "--clearmodifiers", "--", text])
            _debug("Done")
    else:
        _debug("No text to output")

    _debug(f"Total time: {_elapsed():.2f}s")


if __name__ == "__main__":
    main()
